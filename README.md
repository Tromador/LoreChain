# LoreChain

![Project Status](https://img.shields.io/badge/status-WIP-yellow)

**LangChain-based persistent memory layer for creative projects**  
Built for TTRPG lore, story development, and live session context retention â€” but adaptable to any domain where structured memory and semantic retrieval matter.

---

## ğŸ§© Project Overview

**LoreChain** provides:

- ğŸ” A modular LangChain interface with vectorstore-based memory
- ğŸ§  Per-session context management using FAISS + HuggingFace embeddings
- âš¡ï¸ Full GPU acceleration via PyTorch + FAISS-GPU
- ğŸŒ API interaction with OpenAI (GPT-4.1 or later) using `langchain-openai`
- ğŸ–¥ï¸ Web-based input interface (manually typed or programmatically streamed)

Designed for tasks that require long-term knowledge retention, such as:

- ğŸ—ºï¸ Worldbuilding & TTRPG adventure writing
- âœï¸ Multi-turn lore development
- ğŸ¤– Assistant-driven live narrative tools

---

## ğŸš€ Requirements

- Python 3.10+ (tested in WSL Ubuntu)
- Working OpenAI API key (set via config)
- CUDA 12.8+ GPU with PyTorch support (optional but recommended)

---

## âš ï¸ NumPy & FAISS Compatibility Warning

> You **must** use a version of FAISS that is compatible with **NumPy 2.x**

### Why?

- LangChain **requires NumPy 2.x**
- Default FAISS builds from PyPI **are compiled against NumPy 1.x**
- Mixing the two will **break your runtime immediately** (segfaults, memory issues, undefined behaviour)

### âœ… Solution

This project assumes a **custom build of `faiss-gpu` compiled from source with NumPy 2.x support**.

We do **not** include a FAISS build â€” youâ€™ll need to:

- âœ… [Build FAISS from source](https://github.com/facebookresearch/faiss/blob/main/INSTALL.md) with:
  ```bash
  -DFAISS_ENABLE_GPU=ON
  ```
  â€¦and ensure it links against the **same** Python environment where NumPy 2.x is installed.

> âŒ Do **not** downgrade to NumPy 1.x â€” LangChain will break.

---

## âš™ï¸ GPU Acceleration

The system is designed to exploit modern GPU hardware:

- HuggingFace `bge-large-en-v1.5` embeddings (very large model)
- FAISS-GPU for vector similarity
- Transcription + embedding can comfortably run in parallel

> On an RTX 5090, we routinely hit <40% GPU load with both Whisper and embedding active.

If youâ€™re running on lower-spec gear, swap in smaller embedding models or use CPU FAISS instead (with appropriate compile flags).

---

## ğŸ“ Project Structure.

<details> <summary><strong> Click to expand </strong></summary>

```
LoreChain/
â”œâ”€â”€ README.md
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ core.md
â”‚   â”œâ”€â”€ input_interface.md
â”‚   â””â”€â”€ memory.md
â”œâ”€â”€ lc_input_interface/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ langchain_relay.py
â”‚   â”œâ”€â”€ input_providers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”œâ”€â”€ live.py
â”‚   â”‚   â””â”€â”€ manual.py
â”‚   â”œâ”€â”€ lc_core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ bge_embedding.py
â”‚   â”‚   â”œâ”€â”€ chain_manager.py
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ config_sample.py
â”‚   â”‚   â”œâ”€â”€ memory_manager.py
â”‚   â”‚   â””â”€â”€ vectorstore/
â”‚   â”‚       â”œâ”€â”€ index.faiss
â”‚   â”‚       â””â”€â”€ index.pkl
â”‚   â”œâ”€â”€ lc_memory/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ memory_store.py
â”‚   â”‚   â””â”€â”€ session_manager.py
â”‚   â”œâ”€â”€ static/              # Reserved for CSS or JS (empty)
â”‚   â””â”€â”€ templates/
â”‚       â””â”€â”€ input_form.html

```
</details>

---

## ğŸ’¬ Configuration

Your OpenAI API key is stored here:

```python
# lc_core/config.py

OPENAI_API_KEY = "<your-key-here>"
```

This file is `.gitignore`d by default.

---

## âœ… Current Capabilities

| Module         | Status       | Notes                                       |
|----------------|--------------|---------------------------------------------|
| LangChain Core | âœ… Complete   | OpenAI + vectorstore memory                 |
| Memory Store   | âœ… Complete   | FAISS-GPU with per-session document tags    |
| Embeddings     | âœ… Complete   | HuggingFace BGE-large on GPU                |
| Input Web UI   | âœ… Working    | Manual and scripted input supported         |
| LlamaIndex     | ğŸš§ Planned    | Lore preloading system (future enhancement) |
| TTS / Output   | ğŸ§ª Prototype  | TTS not yet looped back into LC             |

---

## ğŸ“¢ Input Source: Discord Chat

You can integrate LoreChain with [**Discord-Transcription-Stack**](https://github.com/Tromador/Discord-Transcription-Stack), which captures clean, diarised chat logs from live Discord voice channels.

That stack is currently using Puppeteer to drive a ChatGPT web session, but will be adapted to use LoreChain directly via API injection in the next dev cycle.

---

## âœ¨ Future Goals

- LlamaIndex-based lore preload with smart retrieval
- TTS module integration (e.g., Bark or Coqui)
- Multi-user session context handling
- Session switching, tagging, history management

---

## ğŸ¤ Attribution

Built by [Tromador](https://github.com/Tromador), an engineer/game master solving actual problems with LLMs instead of playing prompt-jockey games.

No fluff. No magic. Just real tools, running on real iron.

[![License](https://img.shields.io/github/license/Tromador/LoreChain)](https://github.com/Tromador/LoreChain/blob/main/LICENSE)
[![Python](https://img.shields.io/badge/python-3.10%2B-blue.svg)](https://www.python.org/downloads/)
[![LangChain](https://img.shields.io/badge/langchain-enabled-green)](https://github.com/hwchase17/langchain)
[![FAISS](https://img.shields.io/badge/FAISS-GPU--Enabled-brightgreen)](https://github.com/facebookresearch/faiss)
[![Powered by OpenAI](https://img.shields.io/badge/powered%20by-OpenAI-000000?logo=openai&logoColor=white)](https://openai.com)
[![Hugging Face](https://img.shields.io/badge/embeddings-HuggingFace-orange?logo=huggingface&logoColor=white)](https://huggingface.co)

---

## ğŸ“œ License
BSD 3-Clause License â€” Permissive use, with **attribution required**.  

---
